{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"dataR2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Classification\"] = df[\"Classification\"].apply(lambda x: 0 if x == 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Classification\"] = df[\"Classification\"].apply(lambda x: 1 if x == 2 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "111    2\n",
       "112    2\n",
       "113    2\n",
       "114    2\n",
       "115    2\n",
       "Name: Classification, Length: 116, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def przypisz_zero_lub_jeden(x):\n",
    "    if x == 1:\n",
    "        return 0\n",
    "    if x == 2:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Classification\"] = df[\"Classification\"].apply(przypisz_zero_lub_jeden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "111    1\n",
       "112    1\n",
       "113    1\n",
       "114    1\n",
       "115    1\n",
       "Name: Classification, Length: 116, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>HOMA</th>\n",
       "      <th>Leptin</th>\n",
       "      <th>Adiponectin</th>\n",
       "      <th>Resistin</th>\n",
       "      <th>MCP.1</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>70</td>\n",
       "      <td>2.707</td>\n",
       "      <td>0.467409</td>\n",
       "      <td>8.8071</td>\n",
       "      <td>9.702400</td>\n",
       "      <td>7.99585</td>\n",
       "      <td>417.114</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>20.690495</td>\n",
       "      <td>92</td>\n",
       "      <td>3.115</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>5.429285</td>\n",
       "      <td>4.06405</td>\n",
       "      <td>468.786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>23.124670</td>\n",
       "      <td>91</td>\n",
       "      <td>4.498</td>\n",
       "      <td>1.009651</td>\n",
       "      <td>17.9393</td>\n",
       "      <td>22.432040</td>\n",
       "      <td>9.27715</td>\n",
       "      <td>554.697</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>21.367521</td>\n",
       "      <td>77</td>\n",
       "      <td>3.226</td>\n",
       "      <td>0.612725</td>\n",
       "      <td>9.8827</td>\n",
       "      <td>7.169560</td>\n",
       "      <td>12.76600</td>\n",
       "      <td>928.220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>92</td>\n",
       "      <td>3.549</td>\n",
       "      <td>0.805386</td>\n",
       "      <td>6.6994</td>\n",
       "      <td>4.819240</td>\n",
       "      <td>10.57635</td>\n",
       "      <td>773.920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>45</td>\n",
       "      <td>26.850000</td>\n",
       "      <td>92</td>\n",
       "      <td>3.330</td>\n",
       "      <td>0.755688</td>\n",
       "      <td>54.6800</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>10.96000</td>\n",
       "      <td>268.230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>62</td>\n",
       "      <td>26.840000</td>\n",
       "      <td>100</td>\n",
       "      <td>4.530</td>\n",
       "      <td>1.117400</td>\n",
       "      <td>12.4500</td>\n",
       "      <td>21.420000</td>\n",
       "      <td>7.32000</td>\n",
       "      <td>330.160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>65</td>\n",
       "      <td>32.050000</td>\n",
       "      <td>97</td>\n",
       "      <td>5.730</td>\n",
       "      <td>1.370998</td>\n",
       "      <td>61.4800</td>\n",
       "      <td>22.540000</td>\n",
       "      <td>10.33000</td>\n",
       "      <td>314.050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>72</td>\n",
       "      <td>25.590000</td>\n",
       "      <td>82</td>\n",
       "      <td>2.820</td>\n",
       "      <td>0.570392</td>\n",
       "      <td>24.9600</td>\n",
       "      <td>33.750000</td>\n",
       "      <td>3.27000</td>\n",
       "      <td>392.460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>86</td>\n",
       "      <td>27.180000</td>\n",
       "      <td>138</td>\n",
       "      <td>19.910</td>\n",
       "      <td>6.777364</td>\n",
       "      <td>90.2800</td>\n",
       "      <td>14.110000</td>\n",
       "      <td>4.35000</td>\n",
       "      <td>90.090</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age        BMI  Glucose  Insulin      HOMA   Leptin  Adiponectin  \\\n",
       "0     48  23.500000       70    2.707  0.467409   8.8071     9.702400   \n",
       "1     83  20.690495       92    3.115  0.706897   8.8438     5.429285   \n",
       "2     82  23.124670       91    4.498  1.009651  17.9393    22.432040   \n",
       "3     68  21.367521       77    3.226  0.612725   9.8827     7.169560   \n",
       "4     86  21.111111       92    3.549  0.805386   6.6994     4.819240   \n",
       "..   ...        ...      ...      ...       ...      ...          ...   \n",
       "111   45  26.850000       92    3.330  0.755688  54.6800    12.100000   \n",
       "112   62  26.840000      100    4.530  1.117400  12.4500    21.420000   \n",
       "113   65  32.050000       97    5.730  1.370998  61.4800    22.540000   \n",
       "114   72  25.590000       82    2.820  0.570392  24.9600    33.750000   \n",
       "115   86  27.180000      138   19.910  6.777364  90.2800    14.110000   \n",
       "\n",
       "     Resistin    MCP.1  Classification  \n",
       "0     7.99585  417.114               0  \n",
       "1     4.06405  468.786               0  \n",
       "2     9.27715  554.697               0  \n",
       "3    12.76600  928.220               0  \n",
       "4    10.57635  773.920               0  \n",
       "..        ...      ...             ...  \n",
       "111  10.96000  268.230               1  \n",
       "112   7.32000  330.160               1  \n",
       "113  10.33000  314.050               1  \n",
       "114   3.27000  392.460               1  \n",
       "115   4.35000   90.090               1  \n",
       "\n",
       "[116 rows x 10 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.69230769e-01, 2.53850329e-01, 7.09219858e-02, ...,\n",
       "        6.06648498e-02, 2.24659127e-01, 0.00000000e+00],\n",
       "       [9.07692308e-01, 1.14826180e-01, 2.26950355e-01, ...,\n",
       "        1.08258334e-02, 2.55926278e-01, 0.00000000e+00],\n",
       "       [8.92307692e-01, 2.35277707e-01, 2.19858156e-01, ...,\n",
       "        7.69064520e-02, 3.07911729e-01, 0.00000000e+00],\n",
       "       ...,\n",
       "       [6.30769231e-01, 6.76934210e-01, 2.62411348e-01, ...,\n",
       "        9.02522500e-02, 1.62294256e-01, 1.00000000e+00],\n",
       "       [7.38461538e-01, 3.57270833e-01, 1.56028369e-01, ...,\n",
       "        7.60552668e-04, 2.09740790e-01, 1.00000000e+00],\n",
       "       [9.53846154e-01, 4.35949590e-01, 5.53191489e-01, ...,\n",
       "        1.44505007e-02, 2.67742226e-02, 1.00000000e+00]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"Classification\")\n",
    "y = df.loc[:, \"Classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "train_X, test_X, train_y, test_y = train_test_split(StandardScaler().fit_transform(X), y, test_size=0.2, random_state=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X = scaler.fit_transform(train_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(class_weight=\"balanced\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_XX, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = clf.predict(train_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05078012, 0.01463236, 0.57990497, 0.376282  , 0.47950714,\n",
       "       0.44568321, 0.11770698, 0.5844146 , 0.19489603, 0.53950699,\n",
       "       0.06662328, 1.        , 0.12334219, 0.51365301, 0.86710376,\n",
       "       0.73813356, 0.97336868, 0.53334284, 0.2359788 , 0.06417483,\n",
       "       0.03265548, 0.12331994, 0.96108041, 0.71355543, 0.1157622 ,\n",
       "       0.99921495, 0.18470702, 0.04459746, 0.20867923, 0.9500302 ,\n",
       "       0.66797921, 0.44019401, 0.13933298, 0.30983677, 0.17006713,\n",
       "       0.23770687, 0.07349154, 0.1322803 , 0.32932616, 0.05862993,\n",
       "       0.569161  , 0.13164408, 0.61491401, 0.31813909, 0.26102916,\n",
       "       0.93061675, 0.99929095, 0.08667809, 0.98836508, 0.1439798 ,\n",
       "       0.19459749, 0.07753706, 0.13741368, 0.042202  , 0.28690496,\n",
       "       0.18980915, 0.61985818, 0.08252228, 0.66245049, 0.0479743 ,\n",
       "       0.36763347, 0.58805446, 0.98952806, 0.02054025, 0.13171427,\n",
       "       0.89072965, 0.5798369 , 0.603733  , 0.10177182, 0.17069191,\n",
       "       0.99999991, 0.55921665, 0.29061803, 0.23581042, 0.99998734,\n",
       "       0.81146679, 0.05538615, 0.01588925, 0.02934905, 0.02099543,\n",
       "       0.05378593, 0.02717255, 0.27884774, 0.22775549, 0.03292853,\n",
       "       0.97780419, 0.98117723, 0.12603129, 0.43101121, 0.17672178,\n",
       "       0.12916904, 0.10173791])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(train_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94921988, 0.05078012])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(train_X)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prog_odciecia = 0.5\n",
    "np.where(clf.predict_proba(train_X)[:,1] > prog_odciecia, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_odciecia = 0.5\n",
    "predicted_y = np.where(clf.predict_proba(train_X)[:,1] > prog_odciecia, 1, 0) \n",
    "#prÃ³g odciÄ™cia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(train_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36,  4],\n",
       "       [24, 28]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.90      0.72        40\n",
      "           1       0.88      0.54      0.67        52\n",
      "\n",
      "   micro avg       0.70      0.70      0.70        92\n",
      "   macro avg       0.74      0.72      0.69        92\n",
      "weighted avg       0.76      0.70      0.69        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf[0][1]\n",
    "print(classification_report(train_y, predicted_y)) #trafnosc na podstawie raportu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.75      0.71        40\n",
      "           1       0.79      0.73      0.76        52\n",
      "\n",
      "    accuracy                           0.74        92\n",
      "   macro avg       0.74      0.74      0.74        92\n",
      "weighted avg       0.74      0.74      0.74        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_y, predicted_y)) #trafnosc na podstawie raportu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (30 + 38) / (30+38 + 10 +14) #oblicznmy rÄ™cznie \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(accuracy,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensitivity = #TP/TP+FN\n",
    "sensitivity= 30/ (30+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TP | FN\n",
    "#FP | TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- koszt poprawnego zaklasyfikowania chorej osoby i natychmiastowej terapii wyniesie 48,477* #true_positive\n",
    "- koszt poprawnego zaklasyfikowania osoby zdrowej wyniesie 0 #true_negative\n",
    "- koszt niepoprawnego zaklasyfikowania chorej osoby jako zdrowej i znacznie pÃ³Åºniejszego wykrycia nowotworu wyniesie 89,463*#false_positive\n",
    "- koszt niepoprawnego zaklasyfikowania osoby zdrowej jako chorej i skierowania jej na dalsze testy wyniesie 751* #false_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja ktora bierze matrix-a i obliczy koszty(return)\n",
    "# petla for ktora bedzie nam wyznaczala rozne odciecia np od 0.2 do 0.8 -> skok 0.01 \n",
    "# obliczamy koszty dla danego odciecia -> {40: 200000, 40.01: 200002}\n",
    "# na podstawie dictionary chcemy znalezc min -> odp z tego da nam najlepsze odciecie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3895288\n"
     ]
    }
   ],
   "source": [
    "def oblicz_koszty(conf_matrix):\n",
    "    suma = 0\n",
    "    suma += conf_matrix[0][0] * 48477 \n",
    "    suma += conf_matrix[1][0] * 89463\n",
    "    suma += conf_matrix[0][1] * 751\n",
    "    return suma\n",
    "\n",
    "np.array([['TP', \"FN\"],['FP', \"TN\"]])\n",
    "suma = oblicz_koszty(confusion_matrix(train_y, predicted_y))\n",
    "print(suma)\n",
    "    #TP | FN \n",
    "    #FP | TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FP'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([['TP', \"FN\"],['FP', \"TN\"]])\n",
    "a[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lisete =[x for x in range(10)] #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oblicz_predicted_y(prog_odciecia, train_X, clf):\n",
    "    predicted_y = np.where(clf.predict_proba(train_X)[:,1] > prog_odciecia, 1, 0) \n",
    "    return predicted_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "[[ 0 40]\n",
      " [ 0 52]]\n",
      "0.0133\n",
      "[[ 0 40]\n",
      " [ 0 52]]\n",
      "0.0166\n",
      "[[ 1 39]\n",
      " [ 1 51]]\n",
      "0.0198\n",
      "[[ 1 39]\n",
      " [ 1 51]]\n",
      "0.0231\n",
      "[[ 3 37]\n",
      " [ 1 51]]\n",
      "0.0264\n",
      "[[ 3 37]\n",
      " [ 1 51]]\n",
      "0.0297\n",
      "[[ 5 35]\n",
      " [ 1 51]]\n",
      "0.0329\n",
      "[[ 6 34]\n",
      " [ 1 51]]\n",
      "0.0362\n",
      "[[ 7 33]\n",
      " [ 1 51]]\n",
      "0.0395\n",
      "[[ 7 33]\n",
      " [ 1 51]]\n",
      "0.0428\n",
      "[[ 8 32]\n",
      " [ 1 51]]\n",
      "0.0461\n",
      "[[ 9 31]\n",
      " [ 1 51]]\n",
      "0.0493\n",
      "[[10 30]\n",
      " [ 1 51]]\n",
      "0.0526\n",
      "[[11 29]\n",
      " [ 1 51]]\n",
      "0.0559\n",
      "[[13 27]\n",
      " [ 1 51]]\n",
      "0.0592\n",
      "[[14 26]\n",
      " [ 1 51]]\n",
      "0.0624\n",
      "[[14 26]\n",
      " [ 1 51]]\n",
      "0.0657\n",
      "[[15 25]\n",
      " [ 1 51]]\n",
      "0.069\n",
      "[[15 25]\n",
      " [ 2 50]]\n",
      "0.0723\n",
      "[[15 25]\n",
      " [ 2 50]]\n",
      "0.0756\n",
      "[[16 24]\n",
      " [ 2 50]]\n",
      "0.0788\n",
      "[[17 23]\n",
      " [ 2 50]]\n",
      "0.0821\n",
      "[[17 23]\n",
      " [ 2 50]]\n",
      "0.0854\n",
      "[[18 22]\n",
      " [ 2 50]]\n",
      "0.0887\n",
      "[[18 22]\n",
      " [ 3 49]]\n",
      "0.0919\n",
      "[[18 22]\n",
      " [ 3 49]]\n",
      "0.0952\n",
      "[[18 22]\n",
      " [ 3 49]]\n",
      "0.0985\n",
      "[[18 22]\n",
      " [ 3 49]]\n",
      "0.1018\n",
      "[[20 20]\n",
      " [ 3 49]]\n",
      "0.1051\n",
      "[[20 20]\n",
      " [ 3 49]]\n",
      "0.1083\n",
      "[[20 20]\n",
      " [ 3 49]]\n",
      "0.1116\n",
      "[[20 20]\n",
      " [ 3 49]]\n",
      "0.1149\n",
      "[[20 20]\n",
      " [ 3 49]]\n",
      "0.1182\n",
      "[[21 19]\n",
      " [ 4 48]]\n",
      "0.1214\n",
      "[[21 19]\n",
      " [ 4 48]]\n",
      "0.1247\n",
      "[[22 18]\n",
      " [ 5 47]]\n",
      "0.128\n",
      "[[23 17]\n",
      " [ 5 47]]\n",
      "0.1313\n",
      "[[23 17]\n",
      " [ 6 46]]\n",
      "0.1345\n",
      "[[25 15]\n",
      " [ 7 45]]\n",
      "0.1378\n",
      "[[26 14]\n",
      " [ 7 45]]\n",
      "0.1411\n",
      "[[26 14]\n",
      " [ 8 44]]\n",
      "0.1444\n",
      "[[26 14]\n",
      " [ 9 43]]\n",
      "0.1477\n",
      "[[26 14]\n",
      " [ 9 43]]\n",
      "0.1509\n",
      "[[26 14]\n",
      " [ 9 43]]\n",
      "0.1542\n",
      "[[26 14]\n",
      " [ 9 43]]\n",
      "0.1575\n",
      "[[26 14]\n",
      " [ 9 43]]\n",
      "0.1608\n",
      "[[26 14]\n",
      " [ 9 43]]\n",
      "0.164\n",
      "[[26 14]\n",
      " [ 9 43]]\n",
      "0.1673\n",
      "[[26 14]\n",
      " [ 9 43]]\n",
      "0.1706\n",
      "[[27 13]\n",
      " [ 9 43]]\n",
      "0.1739\n",
      "[[28 12]\n",
      " [ 9 43]]\n",
      "0.1772\n",
      "[[29 11]\n",
      " [ 9 43]]\n",
      "0.1804\n",
      "[[29 11]\n",
      " [ 9 43]]\n",
      "0.1837\n",
      "[[29 11]\n",
      " [ 9 43]]\n",
      "0.187\n",
      "[[29 11]\n",
      " [10 42]]\n",
      "0.1903\n",
      "[[29 11]\n",
      " [11 41]]\n",
      "0.1935\n",
      "[[29 11]\n",
      " [11 41]]\n",
      "0.1968\n",
      "[[29 11]\n",
      " [13 39]]\n",
      "0.2001\n",
      "[[29 11]\n",
      " [13 39]]\n",
      "0.2034\n",
      "[[29 11]\n",
      " [13 39]]\n",
      "0.2067\n",
      "[[29 11]\n",
      " [13 39]]\n",
      "0.2099\n",
      "[[29 11]\n",
      " [14 38]]\n",
      "0.2132\n",
      "[[29 11]\n",
      " [14 38]]\n",
      "0.2165\n",
      "[[29 11]\n",
      " [14 38]]\n",
      "0.2198\n",
      "[[29 11]\n",
      " [14 38]]\n",
      "0.223\n",
      "[[29 11]\n",
      " [14 38]]\n",
      "0.2263\n",
      "[[29 11]\n",
      " [14 38]]\n",
      "0.2296\n",
      "[[30 10]\n",
      " [14 38]]\n",
      "0.2329\n",
      "[[30 10]\n",
      " [14 38]]\n",
      "0.2362\n",
      "[[30 10]\n",
      " [16 36]]\n",
      "0.2394\n",
      "[[31  9]\n",
      " [16 36]]\n",
      "0.2427\n",
      "[[31  9]\n",
      " [16 36]]\n",
      "0.246\n",
      "[[31  9]\n",
      " [16 36]]\n",
      "0.2493\n",
      "[[31  9]\n",
      " [16 36]]\n",
      "0.2525\n",
      "[[31  9]\n",
      " [16 36]]\n",
      "0.2558\n",
      "[[31  9]\n",
      " [16 36]]\n",
      "0.2591\n",
      "[[31  9]\n",
      " [16 36]]\n",
      "0.2624\n",
      "[[32  8]\n",
      " [16 36]]\n",
      "0.2657\n",
      "[[32  8]\n",
      " [16 36]]\n",
      "0.2689\n",
      "[[32  8]\n",
      " [16 36]]\n",
      "0.2722\n",
      "[[32  8]\n",
      " [16 36]]\n",
      "0.2755\n",
      "[[32  8]\n",
      " [16 36]]\n",
      "0.2788\n",
      "[[32  8]\n",
      " [16 36]]\n",
      "0.282\n",
      "[[32  8]\n",
      " [17 35]]\n",
      "0.2853\n",
      "[[32  8]\n",
      " [17 35]]\n",
      "0.2886\n",
      "[[33  7]\n",
      " [17 35]]\n",
      "0.2919\n",
      "[[34  6]\n",
      " [17 35]]\n",
      "0.2952\n",
      "[[34  6]\n",
      " [17 35]]\n",
      "0.2984\n",
      "[[34  6]\n",
      " [17 35]]\n",
      "0.3017\n",
      "[[34  6]\n",
      " [17 35]]\n",
      "0.305\n",
      "[[34  6]\n",
      " [17 35]]\n",
      "0.3083\n",
      "[[34  6]\n",
      " [17 35]]\n",
      "0.3115\n",
      "[[35  5]\n",
      " [17 35]]\n",
      "0.3148\n",
      "[[35  5]\n",
      " [17 35]]\n",
      "0.3181\n",
      "[[35  5]\n",
      " [17 35]]\n",
      "0.3214\n",
      "[[36  4]\n",
      " [17 35]]\n",
      "0.3246\n",
      "[[36  4]\n",
      " [17 35]]\n",
      "0.3279\n",
      "[[36  4]\n",
      " [17 35]]\n",
      "0.3312\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.3345\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.3378\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.341\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.3443\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.3476\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.3509\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.3541\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.3574\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.3607\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.364\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.3673\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.3705\n",
      "[[36  4]\n",
      " [19 33]]\n",
      "0.3738\n",
      "[[36  4]\n",
      " [19 33]]\n",
      "0.3771\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.3804\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.3836\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.3869\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.3902\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.3935\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.3968\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.4\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.4033\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.4066\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.4099\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.4131\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.4164\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.4197\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.423\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.4263\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.4295\n",
      "[[36  4]\n",
      " [20 32]]\n",
      "0.4328\n",
      "[[36  4]\n",
      " [21 31]]\n",
      "0.4361\n",
      "[[36  4]\n",
      " [21 31]]\n",
      "0.4394\n",
      "[[36  4]\n",
      " [21 31]]\n",
      "0.4426\n",
      "[[36  4]\n",
      " [22 30]]\n",
      "0.4459\n",
      "[[36  4]\n",
      " [23 29]]\n",
      "0.4492\n",
      "[[36  4]\n",
      " [23 29]]\n",
      "0.4525\n",
      "[[36  4]\n",
      " [23 29]]\n",
      "0.4558\n",
      "[[36  4]\n",
      " [23 29]]\n",
      "0.459\n",
      "[[36  4]\n",
      " [23 29]]\n",
      "0.4623\n",
      "[[36  4]\n",
      " [23 29]]\n",
      "0.4656\n",
      "[[36  4]\n",
      " [23 29]]\n",
      "0.4689\n",
      "[[36  4]\n",
      " [23 29]]\n",
      "0.4721\n",
      "[[36  4]\n",
      " [23 29]]\n",
      "0.4754\n",
      "[[36  4]\n",
      " [23 29]]\n",
      "0.4787\n",
      "[[36  4]\n",
      " [23 29]]\n",
      "0.482\n",
      "[[36  4]\n",
      " [24 28]]\n",
      "0.4853\n",
      "[[36  4]\n",
      " [24 28]]\n",
      "0.4885\n",
      "[[36  4]\n",
      " [24 28]]\n",
      "0.4918\n",
      "[[36  4]\n",
      " [24 28]]\n",
      "0.4951\n",
      "[[36  4]\n",
      " [24 28]]\n",
      "0.4984\n",
      "[[36  4]\n",
      " [24 28]]\n",
      "0.5016\n",
      "[[36  4]\n",
      " [24 28]]\n",
      "0.5049\n",
      "[[36  4]\n",
      " [24 28]]\n",
      "0.5082\n",
      "[[36  4]\n",
      " [24 28]]\n",
      "0.5115\n",
      "[[36  4]\n",
      " [24 28]]\n",
      "0.5147\n",
      "[[36  4]\n",
      " [25 27]]\n",
      "0.518\n",
      "[[36  4]\n",
      " [25 27]]\n",
      "0.5213\n",
      "[[36  4]\n",
      " [25 27]]\n",
      "0.5246\n",
      "[[36  4]\n",
      " [25 27]]\n",
      "0.5279\n",
      "[[36  4]\n",
      " [25 27]]\n",
      "0.5311\n",
      "[[36  4]\n",
      " [25 27]]\n",
      "0.5344\n",
      "[[36  4]\n",
      " [26 26]]\n",
      "0.5377\n",
      "[[36  4]\n",
      " [26 26]]\n",
      "0.541\n",
      "[[36  4]\n",
      " [27 25]]\n",
      "0.5442\n",
      "[[36  4]\n",
      " [27 25]]\n",
      "0.5475\n",
      "[[36  4]\n",
      " [27 25]]\n",
      "0.5508\n",
      "[[36  4]\n",
      " [27 25]]\n",
      "0.5541\n",
      "[[36  4]\n",
      " [27 25]]\n",
      "0.5574\n",
      "[[36  4]\n",
      " [27 25]]\n",
      "0.5606\n",
      "[[36  4]\n",
      " [28 24]]\n",
      "0.5639\n",
      "[[36  4]\n",
      " [28 24]]\n",
      "0.5672\n",
      "[[36  4]\n",
      " [28 24]]\n",
      "0.5705\n",
      "[[36  4]\n",
      " [29 23]]\n",
      "0.5737\n",
      "[[36  4]\n",
      " [29 23]]\n",
      "0.577\n",
      "[[36  4]\n",
      " [29 23]]\n",
      "0.5803\n",
      "[[37  3]\n",
      " [30 22]]\n",
      "0.5836\n",
      "[[37  3]\n",
      " [30 22]]\n",
      "0.5869\n",
      "[[37  3]\n",
      " [31 21]]\n",
      "0.5901\n",
      "[[38  2]\n",
      " [31 21]]\n",
      "0.5934\n",
      "[[38  2]\n",
      " [31 21]]\n",
      "0.5967\n",
      "[[38  2]\n",
      " [31 21]]\n",
      "0.6\n",
      "[[38  2]\n",
      " [31 21]]\n",
      "0.6032\n",
      "[[38  2]\n",
      " [31 21]]\n",
      "0.6065\n",
      "[[38  2]\n",
      " [32 20]]\n",
      "0.6098\n",
      "[[38  2]\n",
      " [32 20]]\n",
      "0.6131\n",
      "[[38  2]\n",
      " [32 20]]\n",
      "0.6164\n",
      "[[39  1]\n",
      " [32 20]]\n",
      "0.6196\n",
      "[[39  1]\n",
      " [32 20]]\n",
      "0.6229\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.6262\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.6295\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.6327\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.636\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.6393\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.6426\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.6459\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.6491\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.6524\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.6557\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.659\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.6622\n",
      "[[39  1]\n",
      " [33 19]]\n",
      "0.6655\n",
      "[[39  1]\n",
      " [34 18]]\n",
      "0.6688\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.6721\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.6754\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.6786\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.6819\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.6852\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.6885\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.6917\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.695\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.6983\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.7016\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.7048\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.7081\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.7114\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.7147\n",
      "[[39  1]\n",
      " [36 16]]\n",
      "0.718\n",
      "[[39  1]\n",
      " [36 16]]\n",
      "0.7212\n",
      "[[39  1]\n",
      " [36 16]]\n",
      "0.7245\n",
      "[[39  1]\n",
      " [36 16]]\n",
      "0.7278\n",
      "[[39  1]\n",
      " [36 16]]\n",
      "0.7311\n",
      "[[39  1]\n",
      " [36 16]]\n",
      "0.7343\n",
      "[[39  1]\n",
      " [36 16]]\n",
      "0.7376\n",
      "[[39  1]\n",
      " [36 16]]\n",
      "0.7409\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7442\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7475\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7507\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.754\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7573\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7606\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7638\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7671\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7704\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7737\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.777\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7802\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7835\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7868\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7901\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7933\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7966\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.7999\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.8032\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.8065\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.8097\n",
      "[[39  1]\n",
      " [37 15]]\n",
      "0.813\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8163\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8196\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8228\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8261\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8294\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8327\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.836\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8392\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8425\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8458\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8491\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8523\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8556\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8589\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8622\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8655\n",
      "[[39  1]\n",
      " [38 14]]\n",
      "0.8687\n",
      "[[39  1]\n",
      " [39 13]]\n",
      "0.872\n",
      "[[39  1]\n",
      " [39 13]]\n",
      "0.8753\n",
      "[[39  1]\n",
      " [39 13]]\n",
      "0.8786\n",
      "[[39  1]\n",
      " [39 13]]\n",
      "0.8818\n",
      "[[39  1]\n",
      " [39 13]]\n",
      "0.8851\n",
      "[[39  1]\n",
      " [39 13]]\n",
      "0.8884\n",
      "[[39  1]\n",
      " [39 13]]\n",
      "0.8917\n",
      "[[39  1]\n",
      " [40 12]]\n",
      "0.8949\n",
      "[[39  1]\n",
      " [40 12]]\n",
      "0.8982\n",
      "[[39  1]\n",
      " [40 12]]\n",
      "0.9015\n",
      "[[39  1]\n",
      " [40 12]]\n",
      "0.9048\n",
      "[[39  1]\n",
      " [40 12]]\n",
      "0.9081\n",
      "[[39  1]\n",
      " [40 12]]\n",
      "0.9113\n",
      "[[39  1]\n",
      " [40 12]]\n",
      "0.9146\n",
      "[[39  1]\n",
      " [40 12]]\n",
      "0.9179\n",
      "[[39  1]\n",
      " [40 12]]\n",
      "0.9212\n",
      "[[39  1]\n",
      " [40 12]]\n",
      "0.9244\n",
      "[[39  1]\n",
      " [40 12]]\n",
      "0.9277\n",
      "[[39  1]\n",
      " [40 12]]\n",
      "0.931\n",
      "[[39  1]\n",
      " [41 11]]\n",
      "0.9343\n",
      "[[39  1]\n",
      " [41 11]]\n",
      "0.9376\n",
      "[[39  1]\n",
      " [41 11]]\n",
      "0.9408\n",
      "[[39  1]\n",
      " [41 11]]\n",
      "0.9441\n",
      "[[39  1]\n",
      " [41 11]]\n",
      "0.9474\n",
      "[[39  1]\n",
      " [41 11]]\n",
      "0.9507\n",
      "[[40  0]\n",
      " [41 11]]\n",
      "0.9539\n",
      "[[40  0]\n",
      " [41 11]]\n",
      "0.9572\n",
      "[[40  0]\n",
      " [41 11]]\n",
      "0.9605\n",
      "[[40  0]\n",
      " [41 11]]\n",
      "0.9638\n",
      "[[40  0]\n",
      " [42 10]]\n",
      "0.9671\n",
      "[[40  0]\n",
      " [42 10]]\n",
      "0.9703\n",
      "[[40  0]\n",
      " [42 10]]\n",
      "0.9736\n",
      "[[40  0]\n",
      " [43  9]]\n",
      "0.9769\n",
      "[[40  0]\n",
      " [43  9]]\n",
      "0.9802\n",
      "[[40  0]\n",
      " [44  8]]\n",
      "0.9834\n",
      "[[40  0]\n",
      " [45  7]]\n",
      "0.9867\n",
      "[[40  0]\n",
      " [45  7]]\n",
      "0.99\n",
      "[[40  0]\n",
      " [47  5]]\n",
      "{0.01: 30040, 0.0133: 30040, 0.0166: 167229, 0.0198: 167229, 0.0231: 262681, 0.0264: 262681, 0.0297: 358133, 0.0329: 405859, 0.0362: 453585, 0.0395: 453585, 0.0428: 501311, 0.0461: 549037, 0.0493: 596763, 0.0526: 644489, 0.0559: 739941, 0.0592: 787667, 0.0624: 787667, 0.0657: 835393, 0.069: 924856, 0.0723: 924856, 0.0756: 972582, 0.0788: 1020308, 0.0821: 1020308, 0.0854: 1068034, 0.0887: 1157497, 0.0919: 1157497, 0.0952: 1157497, 0.0985: 1157497, 0.1018: 1252949, 0.1051: 1252949, 0.1083: 1252949, 0.1116: 1252949, 0.1149: 1252949, 0.1182: 1390138, 0.1214: 1390138, 0.1247: 1527327, 0.128: 1575053, 0.1313: 1664516, 0.1345: 1849431, 0.1378: 1897157, 0.1411: 1986620, 0.1444: 2076083, 0.1477: 2076083, 0.1509: 2076083, 0.1542: 2076083, 0.1575: 2076083, 0.1608: 2076083, 0.164: 2076083, 0.1673: 2076083, 0.1706: 2123809, 0.1739: 2171535, 0.1772: 2219261, 0.1804: 2219261, 0.1837: 2219261, 0.187: 2308724, 0.1903: 2398187, 0.1935: 2398187, 0.1968: 2577113, 0.2001: 2577113, 0.2034: 2577113, 0.2067: 2577113, 0.2099: 2666576, 0.2132: 2666576, 0.2165: 2666576, 0.2198: 2666576, 0.223: 2666576, 0.2263: 2666576, 0.2296: 2714302, 0.2329: 2714302, 0.2362: 2893228, 0.2394: 2940954, 0.2427: 2940954, 0.246: 2940954, 0.2493: 2940954, 0.2525: 2940954, 0.2558: 2940954, 0.2591: 2940954, 0.2624: 2988680, 0.2657: 2988680, 0.2689: 2988680, 0.2722: 2988680, 0.2755: 2988680, 0.2788: 2988680, 0.282: 3078143, 0.2853: 3078143, 0.2886: 3125869, 0.2919: 3173595, 0.2952: 3173595, 0.2984: 3173595, 0.3017: 3173595, 0.305: 3173595, 0.3083: 3173595, 0.3115: 3221321, 0.3148: 3221321, 0.3181: 3221321, 0.3214: 3269047, 0.3246: 3269047, 0.3279: 3269047, 0.3312: 3358510, 0.3345: 3358510, 0.3378: 3358510, 0.341: 3358510, 0.3443: 3358510, 0.3476: 3358510, 0.3509: 3358510, 0.3541: 3358510, 0.3574: 3358510, 0.3607: 3358510, 0.364: 3358510, 0.3673: 3358510, 0.3705: 3447973, 0.3738: 3447973, 0.3771: 3537436, 0.3804: 3537436, 0.3836: 3537436, 0.3869: 3537436, 0.3902: 3537436, 0.3935: 3537436, 0.3968: 3537436, 0.4: 3537436, 0.4033: 3537436, 0.4066: 3537436, 0.4099: 3537436, 0.4131: 3537436, 0.4164: 3537436, 0.4197: 3537436, 0.423: 3537436, 0.4263: 3537436, 0.4295: 3537436, 0.4328: 3626899, 0.4361: 3626899, 0.4394: 3626899, 0.4426: 3716362, 0.4459: 3805825, 0.4492: 3805825, 0.4525: 3805825, 0.4558: 3805825, 0.459: 3805825, 0.4623: 3805825, 0.4656: 3805825, 0.4689: 3805825, 0.4721: 3805825, 0.4754: 3805825, 0.4787: 3805825, 0.482: 3895288, 0.4853: 3895288, 0.4885: 3895288, 0.4918: 3895288, 0.4951: 3895288, 0.4984: 3895288, 0.5016: 3895288, 0.5049: 3895288, 0.5082: 3895288, 0.5115: 3895288, 0.5147: 3984751, 0.518: 3984751, 0.5213: 3984751, 0.5246: 3984751, 0.5279: 3984751, 0.5311: 3984751, 0.5344: 4074214, 0.5377: 4074214, 0.541: 4163677, 0.5442: 4163677, 0.5475: 4163677, 0.5508: 4163677, 0.5541: 4163677, 0.5574: 4163677, 0.5606: 4253140, 0.5639: 4253140, 0.5672: 4253140, 0.5705: 4342603, 0.5737: 4342603, 0.577: 4342603, 0.5803: 4479792, 0.5836: 4479792, 0.5869: 4569255, 0.5901: 4616981, 0.5934: 4616981, 0.5967: 4616981, 0.6: 4616981, 0.6032: 4616981, 0.6065: 4706444, 0.6098: 4706444, 0.6131: 4706444, 0.6164: 4754170, 0.6196: 4754170, 0.6229: 4843633, 0.6262: 4843633, 0.6295: 4843633, 0.6327: 4843633, 0.636: 4843633, 0.6393: 4843633, 0.6426: 4843633, 0.6459: 4843633, 0.6491: 4843633, 0.6524: 4843633, 0.6557: 4843633, 0.659: 4843633, 0.6622: 4843633, 0.6655: 4933096, 0.6688: 5022559, 0.6721: 5022559, 0.6754: 5022559, 0.6786: 5022559, 0.6819: 5022559, 0.6852: 5022559, 0.6885: 5022559, 0.6917: 5022559, 0.695: 5022559, 0.6983: 5022559, 0.7016: 5022559, 0.7048: 5022559, 0.7081: 5022559, 0.7114: 5022559, 0.7147: 5112022, 0.718: 5112022, 0.7212: 5112022, 0.7245: 5112022, 0.7278: 5112022, 0.7311: 5112022, 0.7343: 5112022, 0.7376: 5112022, 0.7409: 5201485, 0.7442: 5201485, 0.7475: 5201485, 0.7507: 5201485, 0.754: 5201485, 0.7573: 5201485, 0.7606: 5201485, 0.7638: 5201485, 0.7671: 5201485, 0.7704: 5201485, 0.7737: 5201485, 0.777: 5201485, 0.7802: 5201485, 0.7835: 5201485, 0.7868: 5201485, 0.7901: 5201485, 0.7933: 5201485, 0.7966: 5201485, 0.7999: 5201485, 0.8032: 5201485, 0.8065: 5201485, 0.8097: 5201485, 0.813: 5290948, 0.8163: 5290948, 0.8196: 5290948, 0.8228: 5290948, 0.8261: 5290948, 0.8294: 5290948, 0.8327: 5290948, 0.836: 5290948, 0.8392: 5290948, 0.8425: 5290948, 0.8458: 5290948, 0.8491: 5290948, 0.8523: 5290948, 0.8556: 5290948, 0.8589: 5290948, 0.8622: 5290948, 0.8655: 5290948, 0.8687: 5380411, 0.872: 5380411, 0.8753: 5380411, 0.8786: 5380411, 0.8818: 5380411, 0.8851: 5380411, 0.8884: 5380411, 0.8917: 5469874, 0.8949: 5469874, 0.8982: 5469874, 0.9015: 5469874, 0.9048: 5469874, 0.9081: 5469874, 0.9113: 5469874, 0.9146: 5469874, 0.9179: 5469874, 0.9212: 5469874, 0.9244: 5469874, 0.9277: 5469874, 0.931: 5559337, 0.9343: 5559337, 0.9376: 5559337, 0.9408: 5559337, 0.9441: 5559337, 0.9474: 5559337, 0.9507: 5607063, 0.9539: 5607063, 0.9572: 5607063, 0.9605: 5607063, 0.9638: 5696526, 0.9671: 5696526, 0.9703: 5696526, 0.9736: 5785989, 0.9769: 5785989, 0.9802: 5875452, 0.9834: 5964915, 0.9867: 5964915, 0.99: 6143841}\n"
     ]
    }
   ],
   "source": [
    "suma_dla_progu = {}\n",
    "for prog_odciecia in np.linspace(0.01, 0.99, num=300):\n",
    "    prog_odciecia = round(prog_odciecia,4)\n",
    "    predicted_y = oblicz_predicted_y(prog_odciecia, train_X, clf)\n",
    "    print(prog_odciecia)\n",
    "    print(confusion_matrix(train_y, predicted_y))\n",
    "    suma = oblicz_koszty(confusion_matrix(train_y, predicted_y))\n",
    "    suma_dla_progu[prog_odciecia] = suma\n",
    "print(suma_dla_progu)\n",
    "    \n",
    "#TP | FN\n",
    "#FP | TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_suma_kosztow = pd.DataFrame({\"prog\": suma_dla_progu.keys(), \"suma\":suma_dla_progu.values()} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prog</th>\n",
       "      <th>suma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>30040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0133</td>\n",
       "      <td>30040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0166</td>\n",
       "      <td>167229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>167229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0231</td>\n",
       "      <td>262681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0264</td>\n",
       "      <td>262681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0297</td>\n",
       "      <td>358133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0329</td>\n",
       "      <td>405859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0362</td>\n",
       "      <td>453585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0395</td>\n",
       "      <td>453585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0428</td>\n",
       "      <td>501311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0461</td>\n",
       "      <td>549037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0493</td>\n",
       "      <td>596763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0526</td>\n",
       "      <td>644489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0559</td>\n",
       "      <td>739941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0592</td>\n",
       "      <td>787667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0624</td>\n",
       "      <td>787667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0657</td>\n",
       "      <td>835393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0690</td>\n",
       "      <td>924856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0723</td>\n",
       "      <td>924856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0756</td>\n",
       "      <td>972582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0788</td>\n",
       "      <td>1020308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0821</td>\n",
       "      <td>1020308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0854</td>\n",
       "      <td>1068034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0887</td>\n",
       "      <td>1157497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0919</td>\n",
       "      <td>1157497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0952</td>\n",
       "      <td>1157497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0985</td>\n",
       "      <td>1157497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.1018</td>\n",
       "      <td>1252949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.1051</td>\n",
       "      <td>1252949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.1083</td>\n",
       "      <td>1252949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.1116</td>\n",
       "      <td>1252949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.1149</td>\n",
       "      <td>1252949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.1182</td>\n",
       "      <td>1390138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.1214</td>\n",
       "      <td>1390138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.1247</td>\n",
       "      <td>1527327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.1280</td>\n",
       "      <td>1575053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.1313</td>\n",
       "      <td>1664516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.1345</td>\n",
       "      <td>1849431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.1378</td>\n",
       "      <td>1897157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.1411</td>\n",
       "      <td>1986620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.1444</td>\n",
       "      <td>2076083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.1477</td>\n",
       "      <td>2076083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.1509</td>\n",
       "      <td>2076083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.1542</td>\n",
       "      <td>2076083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.1575</td>\n",
       "      <td>2076083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.1608</td>\n",
       "      <td>2076083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.1640</td>\n",
       "      <td>2076083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.1673</td>\n",
       "      <td>2076083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.1706</td>\n",
       "      <td>2123809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.1739</td>\n",
       "      <td>2171535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.1772</td>\n",
       "      <td>2219261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.1804</td>\n",
       "      <td>2219261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.1837</td>\n",
       "      <td>2219261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.1870</td>\n",
       "      <td>2308724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.1903</td>\n",
       "      <td>2398187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.1935</td>\n",
       "      <td>2398187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.1968</td>\n",
       "      <td>2577113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.2001</td>\n",
       "      <td>2577113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.2034</td>\n",
       "      <td>2577113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prog     suma\n",
       "0   0.0100    30040\n",
       "1   0.0133    30040\n",
       "2   0.0166   167229\n",
       "3   0.0198   167229\n",
       "4   0.0231   262681\n",
       "5   0.0264   262681\n",
       "6   0.0297   358133\n",
       "7   0.0329   405859\n",
       "8   0.0362   453585\n",
       "9   0.0395   453585\n",
       "10  0.0428   501311\n",
       "11  0.0461   549037\n",
       "12  0.0493   596763\n",
       "13  0.0526   644489\n",
       "14  0.0559   739941\n",
       "15  0.0592   787667\n",
       "16  0.0624   787667\n",
       "17  0.0657   835393\n",
       "18  0.0690   924856\n",
       "19  0.0723   924856\n",
       "20  0.0756   972582\n",
       "21  0.0788  1020308\n",
       "22  0.0821  1020308\n",
       "23  0.0854  1068034\n",
       "24  0.0887  1157497\n",
       "25  0.0919  1157497\n",
       "26  0.0952  1157497\n",
       "27  0.0985  1157497\n",
       "28  0.1018  1252949\n",
       "29  0.1051  1252949\n",
       "30  0.1083  1252949\n",
       "31  0.1116  1252949\n",
       "32  0.1149  1252949\n",
       "33  0.1182  1390138\n",
       "34  0.1214  1390138\n",
       "35  0.1247  1527327\n",
       "36  0.1280  1575053\n",
       "37  0.1313  1664516\n",
       "38  0.1345  1849431\n",
       "39  0.1378  1897157\n",
       "40  0.1411  1986620\n",
       "41  0.1444  2076083\n",
       "42  0.1477  2076083\n",
       "43  0.1509  2076083\n",
       "44  0.1542  2076083\n",
       "45  0.1575  2076083\n",
       "46  0.1608  2076083\n",
       "47  0.1640  2076083\n",
       "48  0.1673  2076083\n",
       "49  0.1706  2123809\n",
       "50  0.1739  2171535\n",
       "51  0.1772  2219261\n",
       "52  0.1804  2219261\n",
       "53  0.1837  2219261\n",
       "54  0.1870  2308724\n",
       "55  0.1903  2398187\n",
       "56  0.1935  2398187\n",
       "57  0.1968  2577113\n",
       "58  0.2001  2577113\n",
       "59  0.2034  2577113"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_suma_kosztow.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2052097ae48>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=df_suma_kosztow['prog'], y= df_suma_kosztow['suma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bierzemy prog 0.2886 bo minimalizujemy bledy 2 stopnia ktore sa najbardziej kosztowne a jednoczesnie dostajemy dostatecznie duzo TP i TN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2886\n",
      "[[19 21]\n",
      " [ 5 47]]\n"
     ]
    }
   ],
   "source": [
    "prog_odciecia = 0.3115\n",
    "predicted_y = oblicz_predicted_y(prog_odciecia, train_X, clf)\n",
    "print(prog_odciecia)\n",
    "print(confusion_matrix(train_y, predicted_y))\n",
    "suma = oblicz_koszty(confusion_matrix(train_y, predicted_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3115\n",
      "[[35  5]\n",
      " [17 35]]\n"
     ]
    }
   ],
   "source": [
    "prog_odciecia = 0.3115\n",
    "predicted_y = oblicz_predicted_y(prog_odciecia, train_X, clf)\n",
    "print(prog_odciecia)\n",
    "print(confusion_matrix(train_y, predicted_y))\n",
    "suma = oblicz_koszty(confusion_matrix(train_y, predicted_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1214\n",
      "[[21 19]\n",
      " [ 4 48]]\n"
     ]
    }
   ],
   "source": [
    "prog_odciecia = 0.1214\n",
    "predicted_y = oblicz_predicted_y(prog_odciecia, train_X, clf)\n",
    "print(prog_odciecia)\n",
    "print(confusion_matrix(train_y, predicted_y))\n",
    "suma = oblicz_koszty(confusion_matrix(train_y, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.53      0.65        40\n",
      "           1       0.72      0.92      0.81        52\n",
      "\n",
      "   micro avg       0.75      0.75      0.75        92\n",
      "   macro avg       0.78      0.72      0.73        92\n",
      "weighted avg       0.77      0.75      0.74        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_y, predicted_y)) #trafnosc na podstawie raportu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.88      0.76        40\n",
      "           1       0.88      0.67      0.76        52\n",
      "\n",
      "   micro avg       0.76      0.76      0.76        92\n",
      "   macro avg       0.77      0.77      0.76        92\n",
      "weighted avg       0.79      0.76      0.76        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_y, predicted_y)) #trafnosc na podstawie raportu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.717391304347826\n",
      "0.475\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = (19 + 47) / (19 +47 + 21 +5) #oblicznmy rÄ™cznie \n",
    "#sensitivity = #TP/TP+FN\n",
    "sensitivity= 19 / (19 +21)\n",
    "print(accuracy)\n",
    "print(sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### po obliczeniu nowego pogu odciÄ™cia, ktÃ³ry minimalizuje koszty dostajemy mniejszÄ… trafnoÅ›Ä‡ i duÅ¼o mniejszÄ… czuÅ‚oÅ›Ä‡, poniewaÅ¼ popeÅ‚niamy wiÄ™cej bÅ‚Ä™dÃ³w 1 stopnia ale minimalizujemy bÅ‚Ä™dy 2 stopnia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
