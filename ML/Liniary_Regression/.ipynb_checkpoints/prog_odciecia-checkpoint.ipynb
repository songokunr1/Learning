{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\patry\\Downloads\\dataR2_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Classification\"] = df[\"Classification\"].apply(lambda x: 0 if x == 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"Classification\"] = df[\"Classification\"].apply(lambda x: 1 if x == 2 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "111    2\n",
       "112    2\n",
       "113    2\n",
       "114    2\n",
       "115    2\n",
       "Name: Classification, Length: 116, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def przypisz_zero_lub_jeden(x):\n",
    "    if x == 1:\n",
    "        return 0\n",
    "    if x == 2:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Classification\"] = df[\"Classification\"].apply(przypisz_zero_lub_jeden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "111    1\n",
       "112    1\n",
       "113    1\n",
       "114    1\n",
       "115    1\n",
       "Name: Classification, Length: 116, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>HOMA</th>\n",
       "      <th>Leptin</th>\n",
       "      <th>Adiponectin</th>\n",
       "      <th>Resistin</th>\n",
       "      <th>MCP.1</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>70</td>\n",
       "      <td>2.707</td>\n",
       "      <td>0.467409</td>\n",
       "      <td>8.8071</td>\n",
       "      <td>9.702400</td>\n",
       "      <td>7.99585</td>\n",
       "      <td>417.114</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>20.690495</td>\n",
       "      <td>92</td>\n",
       "      <td>3.115</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>5.429285</td>\n",
       "      <td>4.06405</td>\n",
       "      <td>468.786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>23.124670</td>\n",
       "      <td>91</td>\n",
       "      <td>4.498</td>\n",
       "      <td>1.009651</td>\n",
       "      <td>17.9393</td>\n",
       "      <td>22.432040</td>\n",
       "      <td>9.27715</td>\n",
       "      <td>554.697</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>21.367521</td>\n",
       "      <td>77</td>\n",
       "      <td>3.226</td>\n",
       "      <td>0.612725</td>\n",
       "      <td>9.8827</td>\n",
       "      <td>7.169560</td>\n",
       "      <td>12.76600</td>\n",
       "      <td>928.220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>92</td>\n",
       "      <td>3.549</td>\n",
       "      <td>0.805386</td>\n",
       "      <td>6.6994</td>\n",
       "      <td>4.819240</td>\n",
       "      <td>10.57635</td>\n",
       "      <td>773.920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>45</td>\n",
       "      <td>26.850000</td>\n",
       "      <td>92</td>\n",
       "      <td>3.330</td>\n",
       "      <td>0.755688</td>\n",
       "      <td>54.6800</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>10.96000</td>\n",
       "      <td>268.230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>62</td>\n",
       "      <td>26.840000</td>\n",
       "      <td>100</td>\n",
       "      <td>4.530</td>\n",
       "      <td>1.117400</td>\n",
       "      <td>12.4500</td>\n",
       "      <td>21.420000</td>\n",
       "      <td>7.32000</td>\n",
       "      <td>330.160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>65</td>\n",
       "      <td>32.050000</td>\n",
       "      <td>97</td>\n",
       "      <td>5.730</td>\n",
       "      <td>1.370998</td>\n",
       "      <td>61.4800</td>\n",
       "      <td>22.540000</td>\n",
       "      <td>10.33000</td>\n",
       "      <td>314.050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>72</td>\n",
       "      <td>25.590000</td>\n",
       "      <td>82</td>\n",
       "      <td>2.820</td>\n",
       "      <td>0.570392</td>\n",
       "      <td>24.9600</td>\n",
       "      <td>33.750000</td>\n",
       "      <td>3.27000</td>\n",
       "      <td>392.460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>86</td>\n",
       "      <td>27.180000</td>\n",
       "      <td>138</td>\n",
       "      <td>19.910</td>\n",
       "      <td>6.777364</td>\n",
       "      <td>90.2800</td>\n",
       "      <td>14.110000</td>\n",
       "      <td>4.35000</td>\n",
       "      <td>90.090</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age        BMI  Glucose  Insulin      HOMA   Leptin  Adiponectin  \\\n",
       "0     48  23.500000       70    2.707  0.467409   8.8071     9.702400   \n",
       "1     83  20.690495       92    3.115  0.706897   8.8438     5.429285   \n",
       "2     82  23.124670       91    4.498  1.009651  17.9393    22.432040   \n",
       "3     68  21.367521       77    3.226  0.612725   9.8827     7.169560   \n",
       "4     86  21.111111       92    3.549  0.805386   6.6994     4.819240   \n",
       "..   ...        ...      ...      ...       ...      ...          ...   \n",
       "111   45  26.850000       92    3.330  0.755688  54.6800    12.100000   \n",
       "112   62  26.840000      100    4.530  1.117400  12.4500    21.420000   \n",
       "113   65  32.050000       97    5.730  1.370998  61.4800    22.540000   \n",
       "114   72  25.590000       82    2.820  0.570392  24.9600    33.750000   \n",
       "115   86  27.180000      138   19.910  6.777364  90.2800    14.110000   \n",
       "\n",
       "     Resistin    MCP.1  Classification  \n",
       "0     7.99585  417.114               0  \n",
       "1     4.06405  468.786               0  \n",
       "2     9.27715  554.697               0  \n",
       "3    12.76600  928.220               0  \n",
       "4    10.57635  773.920               0  \n",
       "..        ...      ...             ...  \n",
       "111  10.96000  268.230               1  \n",
       "112   7.32000  330.160               1  \n",
       "113  10.33000  314.050               1  \n",
       "114   3.27000  392.460               1  \n",
       "115   4.35000   90.090               1  \n",
       "\n",
       "[116 rows x 10 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"Classification\")\n",
    "y = df.loc[:, \"Classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patry\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = clf.predict(train_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27368428, 0.0288283 , 0.80740721, 0.7423257 , 0.61197592,\n",
       "       0.34586766, 0.44415656, 0.79977834, 0.61069566, 0.52799739,\n",
       "       0.26462449, 0.99999896, 0.26386931, 0.78184635, 0.95103199,\n",
       "       0.59115365, 0.97958102, 0.60811157, 0.28944105, 0.14201872,\n",
       "       0.16107563, 0.32262088, 0.94869637, 0.60053643, 0.46340326,\n",
       "       0.98395619, 0.25107412, 0.21058438, 0.54851011, 0.84594772,\n",
       "       0.7753912 , 0.82139948, 0.47284663, 0.58299553, 0.53715743,\n",
       "       0.36660368, 0.2204664 , 0.41386233, 0.75120594, 0.27133501,\n",
       "       0.7789101 , 0.41201772, 0.69865482, 0.57565359, 0.28240245,\n",
       "       0.88234375, 0.99247566, 0.18291537, 0.96792914, 0.52578645,\n",
       "       0.38418257, 0.36785548, 0.52014914, 0.08082248, 0.47955281,\n",
       "       0.29060328, 0.82553374, 0.10515521, 0.89736956, 0.34188862,\n",
       "       0.53499018, 0.5971978 , 0.98662375, 0.06445612, 0.58983113,\n",
       "       0.93549171, 0.82045217, 0.6768272 , 0.23672403, 0.26269134,\n",
       "       0.99998787, 0.86249485, 0.30110034, 0.72308094, 0.99996196,\n",
       "       0.93062896, 0.34174474, 0.0518121 , 0.16466415, 0.06288167,\n",
       "       0.13443635, 0.18557629, 0.49299834, 0.47124622, 0.11617056,\n",
       "       0.93417648, 0.96734072, 0.3741356 , 0.72423393, 0.5222558 ,\n",
       "       0.31320115, 0.27666212])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(train_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72631572, 0.27368428])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(train_X)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(clf.predict_proba(train_X)[:,1] > prog_odciecia, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_odciecia = 0.5\n",
    "predicted_y = np.where(clf.predict_proba(train_X)[:,1] > prog_odciecia, 1, 0) \n",
    "#próg odcięcia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(train_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30, 10],\n",
       "       [14, 38]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.75      0.71        40\n",
      "           1       0.79      0.73      0.76        52\n",
      "\n",
      "    accuracy                           0.74        92\n",
      "   macro avg       0.74      0.74      0.74        92\n",
      "weighted avg       0.74      0.74      0.74        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_y, predicted_y)) #trafnosc na podstawie raportu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (30 + 38) / (30+38 + 10 +14) #oblicznmy ręcznie \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(accuracy,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensitivity = #TP/TP+FN\n",
    "sensitivity= 30/ (30+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TP | FN\n",
    "#FP | TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- koszt poprawnego zaklasyfikowania chorej osoby i natychmiastowej terapii wyniesie 48,477* #true_positive\n",
    "- koszt poprawnego zaklasyfikowania osoby zdrowej wyniesie 0 #true_negative\n",
    "- koszt niepoprawnego zaklasyfikowania chorej osoby jako zdrowej i znacznie późniejszego wykrycia nowotworu wyniesie 89,463*#false_positive\n",
    "- koszt niepoprawnego zaklasyfikowania osoby zdrowej jako chorej i skierowania jej na dalsze testy wyniesie 751* #false_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funkcja ktora bierze matrix-a i obliczy koszty(return)\n",
    "# petla for ktora bedzie nam wyznaczala rozne odciecia np od 0.2 do 0.8 -> skok 0.01 \n",
    "# obliczamy koszty dla danego odciecia -> {40: 200000, 40.01: 200002}\n",
    "# na podstawie dictionary chcemy znalezc min -> odp z tego da nam najlepsze odciecie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233304\n"
     ]
    }
   ],
   "source": [
    "def oblicz_koszty(conf_matrix):\n",
    "    suma = 0\n",
    "    suma += conf_matrix[0][0] * 48477 \n",
    "    suma += conf_matrix[1][0] * 89463\n",
    "    suma += conf_matrix[0][1] * 751\n",
    "    \n",
    "    return suma\n",
    "\n",
    "suma = oblicz_koszty(confusion_matrix(train_y, predicted_y))\n",
    "print(suma)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lisete =[x for x in range(10)] #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2544426\n"
     ]
    }
   ],
   "source": [
    "def oblicz_predicted_y(prog_odciecia, train_X, clf):\n",
    "    predicted_y = np.where(clf.predict_proba(train_X)[:,1] > prog_odciecia, 1, 0) \n",
    "    return predicted_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "[[ 0 40]\n",
      " [ 0 52]]\n",
      "0.0133\n",
      "[[ 0 40]\n",
      " [ 0 52]]\n",
      "0.0166\n",
      "[[ 0 40]\n",
      " [ 0 52]]\n",
      "0.0198\n",
      "[[ 0 40]\n",
      " [ 0 52]]\n",
      "0.0231\n",
      "[[ 0 40]\n",
      " [ 0 52]]\n",
      "0.0264\n",
      "[[ 0 40]\n",
      " [ 0 52]]\n",
      "0.0297\n",
      "[[ 1 39]\n",
      " [ 0 52]]\n",
      "0.0329\n",
      "[[ 1 39]\n",
      " [ 0 52]]\n",
      "0.0362\n",
      "[[ 1 39]\n",
      " [ 0 52]]\n",
      "0.0395\n",
      "[[ 1 39]\n",
      " [ 0 52]]\n",
      "0.0428\n",
      "[[ 1 39]\n",
      " [ 0 52]]\n",
      "0.0461\n",
      "[[ 1 39]\n",
      " [ 0 52]]\n",
      "0.0493\n",
      "[[ 1 39]\n",
      " [ 0 52]]\n",
      "0.0526\n",
      "[[ 1 39]\n",
      " [ 1 51]]\n",
      "0.0559\n",
      "[[ 1 39]\n",
      " [ 1 51]]\n",
      "0.0592\n",
      "[[ 1 39]\n",
      " [ 1 51]]\n",
      "0.0624\n",
      "[[ 1 39]\n",
      " [ 1 51]]\n",
      "0.0657\n",
      "[[ 3 37]\n",
      " [ 1 51]]\n",
      "0.069\n",
      "[[ 3 37]\n",
      " [ 1 51]]\n",
      "0.0723\n",
      "[[ 3 37]\n",
      " [ 1 51]]\n",
      "0.0756\n",
      "[[ 3 37]\n",
      " [ 1 51]]\n",
      "0.0788\n",
      "[[ 3 37]\n",
      " [ 1 51]]\n",
      "0.0821\n",
      "[[ 4 36]\n",
      " [ 1 51]]\n",
      "0.0854\n",
      "[[ 4 36]\n",
      " [ 1 51]]\n",
      "0.0887\n",
      "[[ 4 36]\n",
      " [ 1 51]]\n",
      "0.0919\n",
      "[[ 4 36]\n",
      " [ 1 51]]\n",
      "0.0952\n",
      "[[ 4 36]\n",
      " [ 1 51]]\n",
      "0.0985\n",
      "[[ 4 36]\n",
      " [ 1 51]]\n",
      "0.1018\n",
      "[[ 4 36]\n",
      " [ 1 51]]\n",
      "0.1051\n",
      "[[ 4 36]\n",
      " [ 1 51]]\n",
      "0.1083\n",
      "[[ 5 35]\n",
      " [ 1 51]]\n",
      "0.1116\n",
      "[[ 5 35]\n",
      " [ 1 51]]\n",
      "0.1149\n",
      "[[ 5 35]\n",
      " [ 1 51]]\n",
      "0.1182\n",
      "[[ 6 34]\n",
      " [ 1 51]]\n",
      "0.1214\n",
      "[[ 6 34]\n",
      " [ 1 51]]\n",
      "0.1247\n",
      "[[ 6 34]\n",
      " [ 1 51]]\n",
      "0.128\n",
      "[[ 6 34]\n",
      " [ 1 51]]\n",
      "0.1313\n",
      "[[ 6 34]\n",
      " [ 1 51]]\n",
      "0.1345\n",
      "[[ 7 33]\n",
      " [ 1 51]]\n",
      "0.1378\n",
      "[[ 7 33]\n",
      " [ 1 51]]\n",
      "0.1411\n",
      "[[ 7 33]\n",
      " [ 1 51]]\n",
      "0.1444\n",
      "[[ 8 32]\n",
      " [ 1 51]]\n",
      "0.1477\n",
      "[[ 8 32]\n",
      " [ 1 51]]\n",
      "0.1509\n",
      "[[ 8 32]\n",
      " [ 1 51]]\n",
      "0.1542\n",
      "[[ 8 32]\n",
      " [ 1 51]]\n",
      "0.1575\n",
      "[[ 8 32]\n",
      " [ 1 51]]\n",
      "0.1608\n",
      "[[ 8 32]\n",
      " [ 1 51]]\n",
      "0.164\n",
      "[[ 9 31]\n",
      " [ 1 51]]\n",
      "0.1673\n",
      "[[10 30]\n",
      " [ 1 51]]\n",
      "0.1706\n",
      "[[10 30]\n",
      " [ 1 51]]\n",
      "0.1739\n",
      "[[10 30]\n",
      " [ 1 51]]\n",
      "0.1772\n",
      "[[10 30]\n",
      " [ 1 51]]\n",
      "0.1804\n",
      "[[10 30]\n",
      " [ 1 51]]\n",
      "0.1837\n",
      "[[10 30]\n",
      " [ 2 50]]\n",
      "0.187\n",
      "[[11 29]\n",
      " [ 2 50]]\n",
      "0.1903\n",
      "[[11 29]\n",
      " [ 2 50]]\n",
      "0.1935\n",
      "[[11 29]\n",
      " [ 2 50]]\n",
      "0.1968\n",
      "[[11 29]\n",
      " [ 2 50]]\n",
      "0.2001\n",
      "[[11 29]\n",
      " [ 2 50]]\n",
      "0.2034\n",
      "[[11 29]\n",
      " [ 2 50]]\n",
      "0.2067\n",
      "[[11 29]\n",
      " [ 2 50]]\n",
      "0.2099\n",
      "[[11 29]\n",
      " [ 2 50]]\n",
      "0.2132\n",
      "[[12 28]\n",
      " [ 2 50]]\n",
      "0.2165\n",
      "[[12 28]\n",
      " [ 2 50]]\n",
      "0.2198\n",
      "[[12 28]\n",
      " [ 2 50]]\n",
      "0.223\n",
      "[[13 27]\n",
      " [ 2 50]]\n",
      "0.2263\n",
      "[[13 27]\n",
      " [ 2 50]]\n",
      "0.2296\n",
      "[[13 27]\n",
      " [ 2 50]]\n",
      "0.2329\n",
      "[[13 27]\n",
      " [ 2 50]]\n",
      "0.2362\n",
      "[[13 27]\n",
      " [ 2 50]]\n",
      "0.2394\n",
      "[[14 26]\n",
      " [ 2 50]]\n",
      "0.2427\n",
      "[[14 26]\n",
      " [ 2 50]]\n",
      "0.246\n",
      "[[14 26]\n",
      " [ 2 50]]\n",
      "0.2493\n",
      "[[14 26]\n",
      " [ 2 50]]\n",
      "0.2525\n",
      "[[14 26]\n",
      " [ 3 49]]\n",
      "0.2558\n",
      "[[14 26]\n",
      " [ 3 49]]\n",
      "0.2591\n",
      "[[14 26]\n",
      " [ 3 49]]\n",
      "0.2624\n",
      "[[14 26]\n",
      " [ 3 49]]\n",
      "0.2657\n",
      "[[15 25]\n",
      " [ 5 47]]\n",
      "0.2689\n",
      "[[15 25]\n",
      " [ 5 47]]\n",
      "0.2722\n",
      "[[16 24]\n",
      " [ 5 47]]\n",
      "0.2755\n",
      "[[17 23]\n",
      " [ 5 47]]\n",
      "0.2788\n",
      "[[18 22]\n",
      " [ 5 47]]\n",
      "0.282\n",
      "[[18 22]\n",
      " [ 5 47]]\n",
      "0.2853\n",
      "[[19 21]\n",
      " [ 5 47]]\n",
      "0.2886\n",
      "[[19 21]\n",
      " [ 5 47]]\n",
      "0.2919\n",
      "[[19 21]\n",
      " [ 7 45]]\n",
      "0.2952\n",
      "[[19 21]\n",
      " [ 7 45]]\n",
      "0.2984\n",
      "[[19 21]\n",
      " [ 7 45]]\n",
      "0.3017\n",
      "[[20 20]\n",
      " [ 7 45]]\n",
      "0.305\n",
      "[[20 20]\n",
      " [ 7 45]]\n",
      "0.3083\n",
      "[[20 20]\n",
      " [ 7 45]]\n",
      "0.3115\n",
      "[[20 20]\n",
      " [ 7 45]]\n",
      "0.3148\n",
      "[[20 20]\n",
      " [ 8 44]]\n",
      "0.3181\n",
      "[[20 20]\n",
      " [ 8 44]]\n",
      "0.3214\n",
      "[[20 20]\n",
      " [ 8 44]]\n",
      "0.3246\n",
      "[[21 19]\n",
      " [ 8 44]]\n",
      "0.3279\n",
      "[[21 19]\n",
      " [ 8 44]]\n",
      "0.3312\n",
      "[[21 19]\n",
      " [ 8 44]]\n",
      "0.3345\n",
      "[[21 19]\n",
      " [ 8 44]]\n",
      "0.3378\n",
      "[[21 19]\n",
      " [ 8 44]]\n",
      "0.341\n",
      "[[21 19]\n",
      " [ 8 44]]\n",
      "0.3443\n",
      "[[23 17]\n",
      " [ 8 44]]\n",
      "0.3476\n",
      "[[23 17]\n",
      " [ 9 43]]\n",
      "0.3509\n",
      "[[23 17]\n",
      " [ 9 43]]\n",
      "0.3541\n",
      "[[23 17]\n",
      " [ 9 43]]\n",
      "0.3574\n",
      "[[23 17]\n",
      " [ 9 43]]\n",
      "0.3607\n",
      "[[23 17]\n",
      " [ 9 43]]\n",
      "0.364\n",
      "[[23 17]\n",
      " [ 9 43]]\n",
      "0.3673\n",
      "[[24 16]\n",
      " [ 9 43]]\n",
      "0.3705\n",
      "[[25 15]\n",
      " [ 9 43]]\n",
      "0.3738\n",
      "[[25 15]\n",
      " [ 9 43]]\n",
      "0.3771\n",
      "[[26 14]\n",
      " [ 9 43]]\n",
      "0.3804\n",
      "[[26 14]\n",
      " [ 9 43]]\n",
      "0.3836\n",
      "[[26 14]\n",
      " [ 9 43]]\n",
      "0.3869\n",
      "[[26 14]\n",
      " [10 42]]\n",
      "0.3902\n",
      "[[26 14]\n",
      " [10 42]]\n",
      "0.3935\n",
      "[[26 14]\n",
      " [10 42]]\n",
      "0.3968\n",
      "[[26 14]\n",
      " [10 42]]\n",
      "0.4\n",
      "[[26 14]\n",
      " [10 42]]\n",
      "0.4033\n",
      "[[26 14]\n",
      " [10 42]]\n",
      "0.4066\n",
      "[[26 14]\n",
      " [10 42]]\n",
      "0.4099\n",
      "[[26 14]\n",
      " [10 42]]\n",
      "0.4131\n",
      "[[26 14]\n",
      " [11 41]]\n",
      "0.4164\n",
      "[[27 13]\n",
      " [11 41]]\n",
      "0.4197\n",
      "[[27 13]\n",
      " [11 41]]\n",
      "0.423\n",
      "[[27 13]\n",
      " [11 41]]\n",
      "0.4263\n",
      "[[27 13]\n",
      " [11 41]]\n",
      "0.4295\n",
      "[[27 13]\n",
      " [11 41]]\n",
      "0.4328\n",
      "[[27 13]\n",
      " [11 41]]\n",
      "0.4361\n",
      "[[27 13]\n",
      " [11 41]]\n",
      "0.4394\n",
      "[[27 13]\n",
      " [11 41]]\n",
      "0.4426\n",
      "[[27 13]\n",
      " [11 41]]\n",
      "0.4459\n",
      "[[28 12]\n",
      " [11 41]]\n",
      "0.4492\n",
      "[[28 12]\n",
      " [11 41]]\n",
      "0.4525\n",
      "[[28 12]\n",
      " [11 41]]\n",
      "0.4558\n",
      "[[28 12]\n",
      " [11 41]]\n",
      "0.459\n",
      "[[28 12]\n",
      " [11 41]]\n",
      "0.4623\n",
      "[[28 12]\n",
      " [11 41]]\n",
      "0.4656\n",
      "[[28 12]\n",
      " [12 40]]\n",
      "0.4689\n",
      "[[28 12]\n",
      " [12 40]]\n",
      "0.4721\n",
      "[[29 11]\n",
      " [12 40]]\n",
      "0.4754\n",
      "[[29 11]\n",
      " [13 39]]\n",
      "0.4787\n",
      "[[29 11]\n",
      " [13 39]]\n",
      "0.482\n",
      "[[30 10]\n",
      " [13 39]]\n",
      "0.4853\n",
      "[[30 10]\n",
      " [13 39]]\n",
      "0.4885\n",
      "[[30 10]\n",
      " [13 39]]\n",
      "0.4918\n",
      "[[30 10]\n",
      " [13 39]]\n",
      "0.4951\n",
      "[[30 10]\n",
      " [14 38]]\n",
      "0.4984\n",
      "[[30 10]\n",
      " [14 38]]\n",
      "0.5016\n",
      "[[30 10]\n",
      " [14 38]]\n",
      "0.5049\n",
      "[[30 10]\n",
      " [14 38]]\n",
      "0.5082\n",
      "[[30 10]\n",
      " [14 38]]\n",
      "0.5115\n",
      "[[30 10]\n",
      " [14 38]]\n",
      "0.5147\n",
      "[[30 10]\n",
      " [14 38]]\n",
      "0.518\n",
      "[[30 10]\n",
      " [14 38]]\n",
      "0.5213\n",
      "[[31  9]\n",
      " [14 38]]\n",
      "0.5246\n",
      "[[32  8]\n",
      " [14 38]]\n",
      "0.5279\n",
      "[[32  8]\n",
      " [15 37]]\n",
      "0.5311\n",
      "[[32  8]\n",
      " [16 36]]\n",
      "0.5344\n",
      "[[32  8]\n",
      " [16 36]]\n",
      "0.5377\n",
      "[[33  7]\n",
      " [17 35]]\n",
      "0.541\n",
      "[[33  7]\n",
      " [17 35]]\n",
      "0.5442\n",
      "[[33  7]\n",
      " [17 35]]\n",
      "0.5475\n",
      "[[33  7]\n",
      " [17 35]]\n",
      "0.5508\n",
      "[[33  7]\n",
      " [18 34]]\n",
      "0.5541\n",
      "[[33  7]\n",
      " [18 34]]\n",
      "0.5574\n",
      "[[33  7]\n",
      " [18 34]]\n",
      "0.5606\n",
      "[[33  7]\n",
      " [18 34]]\n",
      "0.5639\n",
      "[[33  7]\n",
      " [18 34]]\n",
      "0.5672\n",
      "[[33  7]\n",
      " [18 34]]\n",
      "0.5705\n",
      "[[33  7]\n",
      " [18 34]]\n",
      "0.5737\n",
      "[[33  7]\n",
      " [18 34]]\n",
      "0.577\n",
      "[[34  6]\n",
      " [18 34]]\n",
      "0.5803\n",
      "[[34  6]\n",
      " [18 34]]\n",
      "0.5836\n",
      "[[35  5]\n",
      " [18 34]]\n",
      "0.5869\n",
      "[[35  5]\n",
      " [18 34]]\n",
      "0.5901\n",
      "[[36  4]\n",
      " [18 34]]\n",
      "0.5934\n",
      "[[36  4]\n",
      " [19 33]]\n",
      "0.5967\n",
      "[[36  4]\n",
      " [19 33]]\n",
      "0.6\n",
      "[[37  3]\n",
      " [19 33]]\n",
      "0.6032\n",
      "[[37  3]\n",
      " [20 32]]\n",
      "0.6065\n",
      "[[37  3]\n",
      " [20 32]]\n",
      "0.6098\n",
      "[[37  3]\n",
      " [21 31]]\n",
      "0.6131\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6164\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6196\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6229\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6262\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6295\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6327\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.636\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6393\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6426\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6459\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6491\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6524\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6557\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.659\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6622\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6655\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6688\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6721\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6754\n",
      "[[37  3]\n",
      " [23 29]]\n",
      "0.6786\n",
      "[[37  3]\n",
      " [24 28]]\n",
      "0.6819\n",
      "[[37  3]\n",
      " [24 28]]\n",
      "0.6852\n",
      "[[37  3]\n",
      " [24 28]]\n",
      "0.6885\n",
      "[[37  3]\n",
      " [24 28]]\n",
      "0.6917\n",
      "[[37  3]\n",
      " [24 28]]\n",
      "0.695\n",
      "[[37  3]\n",
      " [24 28]]\n",
      "0.6983\n",
      "[[37  3]\n",
      " [24 28]]\n",
      "0.7016\n",
      "[[38  2]\n",
      " [24 28]]\n",
      "0.7048\n",
      "[[38  2]\n",
      " [24 28]]\n",
      "0.7081\n",
      "[[38  2]\n",
      " [24 28]]\n",
      "0.7114\n",
      "[[38  2]\n",
      " [24 28]]\n",
      "0.7147\n",
      "[[38  2]\n",
      " [24 28]]\n",
      "0.718\n",
      "[[38  2]\n",
      " [24 28]]\n",
      "0.7212\n",
      "[[38  2]\n",
      " [24 28]]\n",
      "0.7245\n",
      "[[38  2]\n",
      " [26 26]]\n",
      "0.7278\n",
      "[[38  2]\n",
      " [26 26]]\n",
      "0.7311\n",
      "[[38  2]\n",
      " [26 26]]\n",
      "0.7343\n",
      "[[38  2]\n",
      " [26 26]]\n",
      "0.7376\n",
      "[[38  2]\n",
      " [26 26]]\n",
      "0.7409\n",
      "[[38  2]\n",
      " [26 26]]\n",
      "0.7442\n",
      "[[38  2]\n",
      " [27 25]]\n",
      "0.7475\n",
      "[[38  2]\n",
      " [27 25]]\n",
      "0.7507\n",
      "[[38  2]\n",
      " [27 25]]\n",
      "0.754\n",
      "[[38  2]\n",
      " [28 24]]\n",
      "0.7573\n",
      "[[38  2]\n",
      " [28 24]]\n",
      "0.7606\n",
      "[[38  2]\n",
      " [28 24]]\n",
      "0.7638\n",
      "[[38  2]\n",
      " [28 24]]\n",
      "0.7671\n",
      "[[38  2]\n",
      " [28 24]]\n",
      "0.7704\n",
      "[[38  2]\n",
      " [28 24]]\n",
      "0.7737\n",
      "[[38  2]\n",
      " [28 24]]\n",
      "0.777\n",
      "[[38  2]\n",
      " [29 23]]\n",
      "0.7802\n",
      "[[38  2]\n",
      " [30 22]]\n",
      "0.7835\n",
      "[[38  2]\n",
      " [31 21]]\n",
      "0.7868\n",
      "[[38  2]\n",
      " [31 21]]\n",
      "0.7901\n",
      "[[38  2]\n",
      " [31 21]]\n",
      "0.7933\n",
      "[[38  2]\n",
      " [31 21]]\n",
      "0.7966\n",
      "[[38  2]\n",
      " [31 21]]\n",
      "0.7999\n",
      "[[38  2]\n",
      " [32 20]]\n",
      "0.8032\n",
      "[[38  2]\n",
      " [32 20]]\n",
      "0.8065\n",
      "[[38  2]\n",
      " [32 20]]\n",
      "0.8097\n",
      "[[39  1]\n",
      " [32 20]]\n",
      "0.813\n",
      "[[39  1]\n",
      " [32 20]]\n",
      "0.8163\n",
      "[[39  1]\n",
      " [32 20]]\n",
      "0.8196\n",
      "[[39  1]\n",
      " [32 20]]\n",
      "0.8228\n",
      "[[39  1]\n",
      " [34 18]]\n",
      "0.8261\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.8294\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.8327\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.836\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.8392\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.8425\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.8458\n",
      "[[39  1]\n",
      " [35 17]]\n",
      "0.8491\n",
      "[[40  0]\n",
      " [35 17]]\n",
      "0.8523\n",
      "[[40  0]\n",
      " [35 17]]\n",
      "0.8556\n",
      "[[40  0]\n",
      " [35 17]]\n",
      "0.8589\n",
      "[[40  0]\n",
      " [35 17]]\n",
      "0.8622\n",
      "[[40  0]\n",
      " [35 17]]\n",
      "0.8655\n",
      "[[40  0]\n",
      " [36 16]]\n",
      "0.8687\n",
      "[[40  0]\n",
      " [36 16]]\n",
      "0.872\n",
      "[[40  0]\n",
      " [36 16]]\n",
      "0.8753\n",
      "[[40  0]\n",
      " [36 16]]\n",
      "0.8786\n",
      "[[40  0]\n",
      " [36 16]]\n",
      "0.8818\n",
      "[[40  0]\n",
      " [36 16]]\n",
      "0.8851\n",
      "[[40  0]\n",
      " [37 15]]\n",
      "0.8884\n",
      "[[40  0]\n",
      " [37 15]]\n",
      "0.8917\n",
      "[[40  0]\n",
      " [37 15]]\n",
      "0.8949\n",
      "[[40  0]\n",
      " [37 15]]\n",
      "0.8982\n",
      "[[40  0]\n",
      " [38 14]]\n",
      "0.9015\n",
      "[[40  0]\n",
      " [38 14]]\n",
      "0.9048\n",
      "[[40  0]\n",
      " [38 14]]\n",
      "0.9081\n",
      "[[40  0]\n",
      " [38 14]]\n",
      "0.9113\n",
      "[[40  0]\n",
      " [38 14]]\n",
      "0.9146\n",
      "[[40  0]\n",
      " [38 14]]\n",
      "0.9179\n",
      "[[40  0]\n",
      " [38 14]]\n",
      "0.9212\n",
      "[[40  0]\n",
      " [38 14]]\n",
      "0.9244\n",
      "[[40  0]\n",
      " [38 14]]\n",
      "0.9277\n",
      "[[40  0]\n",
      " [38 14]]\n",
      "0.931\n",
      "[[40  0]\n",
      " [39 13]]\n",
      "0.9343\n",
      "[[40  0]\n",
      " [40 12]]\n",
      "0.9376\n",
      "[[40  0]\n",
      " [41 11]]\n",
      "0.9408\n",
      "[[40  0]\n",
      " [41 11]]\n",
      "0.9441\n",
      "[[40  0]\n",
      " [41 11]]\n",
      "0.9474\n",
      "[[40  0]\n",
      " [41 11]]\n",
      "0.9507\n",
      "[[40  0]\n",
      " [42 10]]\n",
      "0.9539\n",
      "[[40  0]\n",
      " [43  9]]\n",
      "0.9572\n",
      "[[40  0]\n",
      " [43  9]]\n",
      "0.9605\n",
      "[[40  0]\n",
      " [43  9]]\n",
      "0.9638\n",
      "[[40  0]\n",
      " [43  9]]\n",
      "0.9671\n",
      "[[40  0]\n",
      " [43  9]]\n",
      "0.9703\n",
      "[[40  0]\n",
      " [45  7]]\n",
      "0.9736\n",
      "[[40  0]\n",
      " [45  7]]\n",
      "0.9769\n",
      "[[40  0]\n",
      " [45  7]]\n",
      "0.9802\n",
      "[[40  0]\n",
      " [46  6]]\n",
      "0.9834\n",
      "[[40  0]\n",
      " [46  6]]\n",
      "0.9867\n",
      "[[40  0]\n",
      " [48  4]]\n",
      "0.99\n",
      "[[40  0]\n",
      " [48  4]]\n",
      "{0.01: 30040, 0.0133: 30040, 0.0166: 30040, 0.0198: 30040, 0.0231: 30040, 0.0264: 30040, 0.0297: 77766, 0.0329: 77766, 0.0362: 77766, 0.0395: 77766, 0.0428: 77766, 0.0461: 77766, 0.0493: 77766, 0.0526: 167229, 0.0559: 167229, 0.0592: 167229, 0.0624: 167229, 0.0657: 262681, 0.069: 262681, 0.0723: 262681, 0.0756: 262681, 0.0788: 262681, 0.0821: 310407, 0.0854: 310407, 0.0887: 310407, 0.0919: 310407, 0.0952: 310407, 0.0985: 310407, 0.1018: 310407, 0.1051: 310407, 0.1083: 358133, 0.1116: 358133, 0.1149: 358133, 0.1182: 405859, 0.1214: 405859, 0.1247: 405859, 0.128: 405859, 0.1313: 405859, 0.1345: 453585, 0.1378: 453585, 0.1411: 453585, 0.1444: 501311, 0.1477: 501311, 0.1509: 501311, 0.1542: 501311, 0.1575: 501311, 0.1608: 501311, 0.164: 549037, 0.1673: 596763, 0.1706: 596763, 0.1739: 596763, 0.1772: 596763, 0.1804: 596763, 0.1837: 686226, 0.187: 733952, 0.1903: 733952, 0.1935: 733952, 0.1968: 733952, 0.2001: 733952, 0.2034: 733952, 0.2067: 733952, 0.2099: 733952, 0.2132: 781678, 0.2165: 781678, 0.2198: 781678, 0.223: 829404, 0.2263: 829404, 0.2296: 829404, 0.2329: 829404, 0.2362: 829404, 0.2394: 877130, 0.2427: 877130, 0.246: 877130, 0.2493: 877130, 0.2525: 966593, 0.2558: 966593, 0.2591: 966593, 0.2624: 966593, 0.2657: 1193245, 0.2689: 1193245, 0.2722: 1240971, 0.2755: 1288697, 0.2788: 1336423, 0.282: 1336423, 0.2853: 1384149, 0.2886: 1384149, 0.2919: 1563075, 0.2952: 1563075, 0.2984: 1563075, 0.3017: 1610801, 0.305: 1610801, 0.3083: 1610801, 0.3115: 1610801, 0.3148: 1700264, 0.3181: 1700264, 0.3214: 1700264, 0.3246: 1747990, 0.3279: 1747990, 0.3312: 1747990, 0.3345: 1747990, 0.3378: 1747990, 0.341: 1747990, 0.3443: 1843442, 0.3476: 1932905, 0.3509: 1932905, 0.3541: 1932905, 0.3574: 1932905, 0.3607: 1932905, 0.364: 1932905, 0.3673: 1980631, 0.3705: 2028357, 0.3738: 2028357, 0.3771: 2076083, 0.3804: 2076083, 0.3836: 2076083, 0.3869: 2165546, 0.3902: 2165546, 0.3935: 2165546, 0.3968: 2165546, 0.4: 2165546, 0.4033: 2165546, 0.4066: 2165546, 0.4099: 2165546, 0.4131: 2255009, 0.4164: 2302735, 0.4197: 2302735, 0.423: 2302735, 0.4263: 2302735, 0.4295: 2302735, 0.4328: 2302735, 0.4361: 2302735, 0.4394: 2302735, 0.4426: 2302735, 0.4459: 2350461, 0.4492: 2350461, 0.4525: 2350461, 0.4558: 2350461, 0.459: 2350461, 0.4623: 2350461, 0.4656: 2439924, 0.4689: 2439924, 0.4721: 2487650, 0.4754: 2577113, 0.4787: 2577113, 0.482: 2624839, 0.4853: 2624839, 0.4885: 2624839, 0.4918: 2624839, 0.4951: 2714302, 0.4984: 2714302, 0.5016: 2714302, 0.5049: 2714302, 0.5082: 2714302, 0.5115: 2714302, 0.5147: 2714302, 0.518: 2714302, 0.5213: 2762028, 0.5246: 2809754, 0.5279: 2899217, 0.5311: 2988680, 0.5344: 2988680, 0.5377: 3125869, 0.541: 3125869, 0.5442: 3125869, 0.5475: 3125869, 0.5508: 3215332, 0.5541: 3215332, 0.5574: 3215332, 0.5606: 3215332, 0.5639: 3215332, 0.5672: 3215332, 0.5705: 3215332, 0.5737: 3215332, 0.577: 3263058, 0.5803: 3263058, 0.5836: 3310784, 0.5869: 3310784, 0.5901: 3358510, 0.5934: 3447973, 0.5967: 3447973, 0.6: 3495699, 0.6032: 3585162, 0.6065: 3585162, 0.6098: 3674625, 0.6131: 3853551, 0.6164: 3853551, 0.6196: 3853551, 0.6229: 3853551, 0.6262: 3853551, 0.6295: 3853551, 0.6327: 3853551, 0.636: 3853551, 0.6393: 3853551, 0.6426: 3853551, 0.6459: 3853551, 0.6491: 3853551, 0.6524: 3853551, 0.6557: 3853551, 0.659: 3853551, 0.6622: 3853551, 0.6655: 3853551, 0.6688: 3853551, 0.6721: 3853551, 0.6754: 3853551, 0.6786: 3943014, 0.6819: 3943014, 0.6852: 3943014, 0.6885: 3943014, 0.6917: 3943014, 0.695: 3943014, 0.6983: 3943014, 0.7016: 3990740, 0.7048: 3990740, 0.7081: 3990740, 0.7114: 3990740, 0.7147: 3990740, 0.718: 3990740, 0.7212: 3990740, 0.7245: 4169666, 0.7278: 4169666, 0.7311: 4169666, 0.7343: 4169666, 0.7376: 4169666, 0.7409: 4169666, 0.7442: 4259129, 0.7475: 4259129, 0.7507: 4259129, 0.754: 4348592, 0.7573: 4348592, 0.7606: 4348592, 0.7638: 4348592, 0.7671: 4348592, 0.7704: 4348592, 0.7737: 4348592, 0.777: 4438055, 0.7802: 4527518, 0.7835: 4616981, 0.7868: 4616981, 0.7901: 4616981, 0.7933: 4616981, 0.7966: 4616981, 0.7999: 4706444, 0.8032: 4706444, 0.8065: 4706444, 0.8097: 4754170, 0.813: 4754170, 0.8163: 4754170, 0.8196: 4754170, 0.8228: 4933096, 0.8261: 5022559, 0.8294: 5022559, 0.8327: 5022559, 0.836: 5022559, 0.8392: 5022559, 0.8425: 5022559, 0.8458: 5022559, 0.8491: 5070285, 0.8523: 5070285, 0.8556: 5070285, 0.8589: 5070285, 0.8622: 5070285, 0.8655: 5159748, 0.8687: 5159748, 0.872: 5159748, 0.8753: 5159748, 0.8786: 5159748, 0.8818: 5159748, 0.8851: 5249211, 0.8884: 5249211, 0.8917: 5249211, 0.8949: 5249211, 0.8982: 5338674, 0.9015: 5338674, 0.9048: 5338674, 0.9081: 5338674, 0.9113: 5338674, 0.9146: 5338674, 0.9179: 5338674, 0.9212: 5338674, 0.9244: 5338674, 0.9277: 5338674, 0.931: 5428137, 0.9343: 5517600, 0.9376: 5607063, 0.9408: 5607063, 0.9441: 5607063, 0.9474: 5607063, 0.9507: 5696526, 0.9539: 5785989, 0.9572: 5785989, 0.9605: 5785989, 0.9638: 5785989, 0.9671: 5785989, 0.9703: 5964915, 0.9736: 5964915, 0.9769: 5964915, 0.9802: 6054378, 0.9834: 6054378, 0.9867: 6233304, 0.99: 6233304}\n"
     ]
    }
   ],
   "source": [
    "suma_dla_progu = {}\n",
    "for prog_odciecia in np.linspace(0.01, 0.99, num=300):\n",
    "    prog_odciecia = round(prog_odciecia,4)\n",
    "    predicted_y = oblicz_predicted_y(prog_odciecia, train_X, clf)\n",
    "    print(prog_odciecia)\n",
    "    print(confusion_matrix(train_y, predicted_y))\n",
    "    suma = oblicz_koszty(confusion_matrix(train_y, predicted_y))\n",
    "    suma_dla_progu[prog_odciecia] = suma\n",
    "print(suma_dla_progu)\n",
    "    \n",
    "#TP | FN\n",
    "#FP | TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_suma_kosztow = pd.DataFrame({\"prog\": suma_dla_progu.keys(), \"suma\":suma_dla_progu.values()} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prog</th>\n",
       "      <th>suma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>30040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0133</td>\n",
       "      <td>30040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0166</td>\n",
       "      <td>30040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>30040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0231</td>\n",
       "      <td>30040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>5964915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.9802</td>\n",
       "      <td>6054378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.9834</td>\n",
       "      <td>6054378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.9867</td>\n",
       "      <td>6233304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.9900</td>\n",
       "      <td>6233304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prog     suma\n",
       "0    0.0100    30040\n",
       "1    0.0133    30040\n",
       "2    0.0166    30040\n",
       "3    0.0198    30040\n",
       "4    0.0231    30040\n",
       "..      ...      ...\n",
       "295  0.9769  5964915\n",
       "296  0.9802  6054378\n",
       "297  0.9834  6054378\n",
       "298  0.9867  6233304\n",
       "299  0.9900  6233304\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_suma_kosztow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='prog', ylabel='suma'>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=df_suma_kosztow['prog'], y= df_suma_kosztow['suma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bierzemy prog 0.2886 bo minimalizujemy bledy 2 stopnia ktore sa najbardziej kosztowne a jednoczesnie dostajemy dostatecznie duzo TP i TN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2886\n",
      "[[19 21]\n",
      " [ 5 47]]\n"
     ]
    }
   ],
   "source": [
    "prog_odciecia = 0.2886\n",
    "predicted_y = oblicz_predicted_y(prog_odciecia, train_X, clf)\n",
    "print(prog_odciecia)\n",
    "print(confusion_matrix(train_y, predicted_y))\n",
    "suma = oblicz_koszty(confusion_matrix(train_y, predicted_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.47      0.59        40\n",
      "           1       0.69      0.90      0.78        52\n",
      "\n",
      "    accuracy                           0.72        92\n",
      "   macro avg       0.74      0.69      0.69        92\n",
      "weighted avg       0.73      0.72      0.70        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_y, predicted_y)) #trafnosc na podstawie raportu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.717391304347826\n",
      "0.475\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = (19 + 47) / (19 +47 + 21 +5) #oblicznmy ręcznie \n",
    "#sensitivity = #TP/TP+FN\n",
    "sensitivity= 19 / (19 +21)\n",
    "print(accuracy)\n",
    "print(sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### po obliczeniu nowego pogu odcięcia, który minimalizuje koszty dostajemy mniejszą trafność i dużo mniejszą czułość, ponieważ popełniamy więcej błędów 1 stopnia ale minimalizujemy błędy 2 stopnia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
